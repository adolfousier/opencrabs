# OpenCrabs Configuration File
# Copy this file to one of these locations:
#   - Linux/macOS: ~/.opencrabs/config.toml
#   - Windows: %APPDATA%\opencrabs\config.toml or opencrabs\config.toml
#
# IMPORTANT: API keys should NOT be stored here!
# Instead, store API keys in keys.toml (chmod 600) for security:
#   - ~/.opencrabs/keys.toml
# Keys in keys.toml take priority over this file.

[database]
# Database file location (stores conversation history)
# path = "~/.opencrabs/opencrabs.db"  # Default; only override if needed

[providers]
# ========================================
# Custom: OpenAI-Compatible Provider (Local LLMs, and any OpenAI Compatible model)
# ========================================
# Use this for LM Studio, Ollama, LocalAI, etc.
[providers.custom]
enabled = true
base_url = "http://localhost:1234/v1/chat/completions"  # LM Studio default
models = ["kimi-k2.5", "glm-5", "MiniMax-M2.5", "qwen3-coder", "gpt-oss-120b", "llama-4-70B", "mistral-Large-3", "qwen3-coder-next"]

# â­ IMPORTANT: Set this to match the model name loaded in LM Studio!
# Common examples:
#   - qwen2.5-coder-7b-instruct
#   - codellama-7b-instruct
#   - deepseek-coder-6.7b-instruct
#   - llama-3.2-1b-instruct
default_model = "qwen2.5-coder-7b-instruct"

# Other local LLM servers:
# Ollama: base_url = "http://localhost:11434/v1/chat/completions"
# LocalAI: base_url = "http://localhost:8080/v1/chat/completions"

# ========================================
# Advanced: Multiple Named Custom Providers
# ========================================
# Want multiple custom providers and switch between them via /models?
# Use named sections instead of [providers.custom]:
#
# [providers.custom.lm_studio]
# enabled = true
# base_url = "http://localhost:1234/v1/chat/completions"
# default_model = "qwen2.5-coder-7b-instruct"
# models = ["qwen2.5-coder-7b-instruct", "llama-3-8B"]
#
# [providers.custom.ollama]
# enabled = false
# base_url = "http://localhost:11434/v1/chat/completions"
# default_model = "mistral"
# models = ["mistral", "llama3", "codellama"]

# ========================================
# Official OpenAI Provider
# ========================================
[providers.openai]
enabled = false
default_model = "gpt-4o"  # Optional: override default model

# ========================================
# Anthropic Provider (Claude)
# ========================================
[providers.anthropic]
enabled = false
default_model = "claude-sonnet-4-6"  # Optional: override default

# ========================================
# OpenRouter Provider (100+ models via OpenAI-compatible API)
# ========================================
[providers.openrouter]
enabled = false
base_url = "https://openrouter.ai/api/v1/chat/completions"
default_model = "qwen/qwen3-coder-next"  # Many options at openrouter.ai/models

# ========================================
# Minimax Provider (Chinese AI, OpenAI-compatible)
# ========================================
# Note: Minimax does NOT have a /models endpoint, so add models manually
[providers.minimax]
enabled = false
base_url = "https://api.minimax.io/v1"
default_model = "MiniMax-M2.5"
models = ["MiniMax-M2.5", "MiniMax-M2.1", "MiniMax-Text-01"]

# ========================================
# STT (Speech-to-Text) Providers
# ========================================
# Groq Whisper for transcription
[providers.stt.groq]
enabled = false
default_model = "whisper-large-v3-turbo"

# ========================================
# TTS (Text-to-Speech) Providers
# ========================================
# OpenAI TTS for voice output
[providers.tts.openai]
enabled = false
default_model = "gpt-4o-mini-tts"
voice = "ash"             # TTS voice name
model = "gpt-4o-mini-tts" # TTS model

# ========================================
# Tips for Using Local LLMs
# ========================================
# 1. Make sure LM Studio is running before starting OpenCrabs
# 2. Load a model in LM Studio first
# 3. Set default_model to EXACTLY match the model name shown in LM Studio
# 4. Increase context length in LM Studio if you get overflow errors:
#    - Recommended: 8192 or higher
#    - Location: LM Studio > Model Settings > Context Length

# ==================================================
# Channels (Telegram / WhatsApp / Slack / Discord)
# ==================================================

[channels.whatsapp]
enabled = false
allowed_phones = ["+15551234567"]

[channels.discord]
enabled = false
allowed_channels = ["channel_id"]   # Where the bot operates (empty = all channels)
allowed_users = [123456789012345]   # Who the bot replies to (empty = everyone)

[channels.telegram]
enabled = false
allowed_users = [123456789]         # Who the bot replies to (empty = everyone)

[channels.slack]
enabled = false
allowed_channels = ["C12345678"]    # Where the bot operates (empty = all channels)
allowed_ids = ["U12345678"]         # Who the bot replies to (empty = everyone)

# ========================================
# Agent-to-Agent (A2A) Protocol
# ========================================
# Enables HTTP gateway for peer-to-peer agent communication.
# Other A2A-compatible agents can send tasks, collaborate, and debate.
[a2a]
enabled = false
bind = "127.0.0.1"     # Loopback only by default for security
port = 18790            # A2A gateway port
# CORS allowed origins (empty = no cross-origin requests allowed)
# allowed_origins = ["http://localhost:3000"]

# ========================================
# Web Search Providers (default to free Duck Duck Go, no need additional web search provider)
# ========================================

[providers.web_search.exa]
enabled = true
# Its free up to 1000 requests. API key goes in keys.toml: [providers.web_search.exa] api_key = "..."

[providers.web_search.brave]
enabled = false
# Its free up to 1000 requests. API key goes in keys.toml: [providers.web_search.brave] api_key = "..."
